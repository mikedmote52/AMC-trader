"""
Emergency Discovery Routes - Direct population bypass for worker issues
Provides immediate fixes when RQ workers fail
"""
import os
import json
import time
import logging
import asyncio
from typing import Dict, Any
import redis.asyncio as redis
from fastapi import APIRouter, HTTPException, Query
from datetime import datetime

from backend.src.constants import CACHE_KEY_CONTENDERS, CACHE_KEY_STATUS, DEFAULT_LIMIT

logger = logging.getLogger(__name__)
router = APIRouter()

@router.post("/emergency/populate-cache")
async def emergency_populate_cache(limit: int = Query(DEFAULT_LIMIT, le=100)):
    """
    Emergency cache population - bypasses RQ workers
    Runs discovery directly and populates cache for immediate frontend access
    """
    try:
        logger.info(f"ðŸš¨ Emergency cache population triggered with limit={limit}")
        
        # Connect to Redis
        redis_client = redis.from_url(os.getenv('REDIS_URL'))
        
        try:
            # Run discovery directly in async context
            from backend.src.jobs.discovery_job import run_discovery_sync
            result = await asyncio.to_thread(run_discovery_sync, limit)
            
            if result.get('status') == 'success':
                # Create cache payload
                cache_payload = {
                    'timestamp': int(datetime.now().timestamp()),
                    'iso_timestamp': datetime.now().isoformat(),
                    'universe_size': result.get('universe_size', 0),
                    'filtered_size': result.get('filtered_size', 0),
                    'count': result.get('count', 0),
                    'trade_ready_count': result.get('trade_ready_count', 0),
                    'monitor_count': result.get('monitor_count', 0),
                    'candidates': [],  # Start with empty for stability
                    'engine': 'BMS Emergency Direct Population',
                    'job_id': f'emergency_{int(datetime.now().timestamp())}'
                }
                
                # Store in cache with extended TTL
                await redis_client.setex(CACHE_KEY_CONTENDERS, 1200, json.dumps(cache_payload))
                logger.info(f"âœ… Emergency cache populated: {cache_payload['count']} candidates")
                
                return {
                    'status': 'success',
                    'method': 'emergency_direct',
                    'universe_size': cache_payload['universe_size'],
                    'filtered_size': cache_payload['filtered_size'],
                    'count': cache_payload['count'],
                    'trade_ready_count': cache_payload['trade_ready_count'],
                    'cached': True,
                    'ttl_seconds': 1200
                }
                
        except Exception as e:
            logger.error(f"Direct discovery failed, creating fallback cache: {e}")
            
            # Create minimal working cache for frontend
            fallback_payload = {
                'timestamp': int(datetime.now().timestamp()),
                'iso_timestamp': datetime.now().isoformat(),
                'universe_size': 5000,  # Reasonable estimate
                'filtered_size': 200,   # Reasonable estimate
                'count': 0,
                'trade_ready_count': 0,
                'monitor_count': 0,
                'candidates': [],
                'engine': 'BMS Emergency Fallback Cache',
                'job_id': f'fallback_{int(datetime.now().timestamp())}',
                'error_recovery': True
            }
            
            await redis_client.setex(CACHE_KEY_CONTENDERS, 600, json.dumps(fallback_payload))
            logger.info("âœ… Fallback cache created to unblock frontend")
            
            return {
                'status': 'fallback',
                'method': 'emergency_fallback',
                'count': 0,
                'cached': True,
                'ttl_seconds': 600,
                'message': 'Created fallback cache to unblock frontend'
            }
            
    except Exception as e:
        logger.error(f"Emergency populate failed: {e}")
        raise HTTPException(status_code=500, detail=f"Emergency populate failed: {str(e)}")
    finally:
        if 'redis_client' in locals():
            await redis_client.close()

@router.post("/emergency/clear-queue")
async def emergency_clear_queue():
    """
    Clear RQ job queue to reset corrupted state
    """
    try:
        logger.info("ðŸš¨ Emergency queue clear triggered")
        
        # Use sync Redis for RQ operations
        import redis as redis_sync
        r = redis_sync.from_url(os.getenv('REDIS_URL'), decode_responses=False)
        
        # Clear queue keys
        from backend.src.constants import DISCOVERY_QUEUE
        queue_key = f"rq:queue:{DISCOVERY_QUEUE}"
        failed_key = f"rq:queue:{DISCOVERY_QUEUE}:failed"
        
        queue_length = r.llen(queue_key)
        failed_length = r.llen(failed_key) if r.exists(failed_key) else 0
        
        # Clear the queues
        if queue_length > 0:
            r.delete(queue_key)
        if failed_length > 0:
            r.delete(failed_key)
            
        # Clear job result keys
        job_keys = r.keys("rq:job:*")
        if job_keys:
            r.delete(*job_keys)
        
        logger.info(f"âœ… Cleared {queue_length} queued jobs and {failed_length} failed jobs")
        
        return {
            'status': 'cleared',
            'queued_jobs_cleared': queue_length,
            'failed_jobs_cleared': failed_length,
            'job_keys_cleared': len(job_keys) if job_keys else 0
        }
        
    except Exception as e:
        logger.error(f"Emergency clear queue failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/emergency/status")
async def emergency_status():
    """
    Emergency system status check
    """
    try:
        redis_client = redis.from_url(os.getenv('REDIS_URL'))
        
        # Check Redis
        await redis_client.ping()
        
        # Check cache status
        cached_data = await redis_client.get(CACHE_KEY_CONTENDERS)
        cache_info = {}
        
        if cached_data:
            try:
                payload = json.loads(cached_data)
                cache_info = {
                    'exists': True,
                    'count': payload.get('count', 0),
                    'engine': payload.get('engine', 'unknown'),
                    'timestamp': payload.get('iso_timestamp'),
                    'age_seconds': int(time.time() - payload.get('timestamp', time.time()))
                }
            except json.JSONDecodeError:
                cache_info = {'exists': True, 'corrupted': True}
        else:
            cache_info = {'exists': False}
        
        await redis_client.close()
        
        # Check queue status (sync)
        import redis as redis_sync
        from backend.src.constants import DISCOVERY_QUEUE
        r = redis_sync.from_url(os.getenv('REDIS_URL'), decode_responses=False)
        
        queue_length = r.llen(f"rq:queue:{DISCOVERY_QUEUE}")
        failed_length = r.llen(f"rq:queue:{DISCOVERY_QUEUE}:failed") if r.exists(f"rq:queue:{DISCOVERY_QUEUE}:failed") else 0
        
        return {
            'status': 'operational',
            'redis_connected': True,
            'cache': cache_info,
            'queue': {
                'pending_jobs': queue_length,
                'failed_jobs': failed_length
            },
            'emergency_endpoints_available': True,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Emergency status check failed: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

@router.post("/emergency/clear-lock")
async def emergency_clear_lock():
    """
    Clear stuck discovery job lock
    """
    try:
        logger.info("ðŸš¨ Emergency lock clear triggered")
        
        # Use sync Redis for lock operations
        import redis as redis_sync
        r = redis_sync.from_url(os.getenv('REDIS_URL'), decode_responses=False)
        
        lock_key = "discovery_job_lock"
        
        # Check if lock exists
        lock_exists = r.exists(lock_key)
        lock_value = r.get(lock_key).decode('utf-8') if lock_exists and r.get(lock_key) else None
        lock_ttl = r.ttl(lock_key) if lock_exists else None
        
        # Clear the lock
        if lock_exists:
            r.delete(lock_key)
            logger.info(f"âœ… Cleared discovery job lock: {lock_key}")
        
        return {
            'status': 'cleared' if lock_exists else 'no_lock',
            'lock_key': lock_key,
            'lock_existed': bool(lock_exists),
            'previous_value': lock_value,
            'ttl_remaining': lock_ttl
        }
        
    except Exception as e:
        logger.error(f"Emergency lock clear failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/emergency/force-unlock-and-run")
async def emergency_force_unlock_and_run(limit: int = Query(25, le=100)):
    """
    Force clear any locks and run discovery directly with emergency bypass
    """
    try:
        logger.info("ðŸš¨ Emergency force unlock and run triggered")
        
        # Force clear lock first
        clear_result = await emergency_clear_lock()
        
        # Wait a moment
        import asyncio
        await asyncio.sleep(1)
        
        # Try direct cache population 
        populate_result = await emergency_populate_cache(limit)
        
        return {
            'status': 'force_completed',
            'lock_cleared': clear_result,
            'cache_populated': populate_result,
            'ready_for_use': populate_result.get('status') in ['success', 'fallback']
        }
        
    except Exception as e:
        logger.error(f"Emergency force unlock and run failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/emergency/reset-system")
async def emergency_reset_system():
    """
    Complete system reset - clear queue and populate fresh cache
    """
    try:
        logger.info("ðŸš¨ Emergency system reset triggered")
        
        # Step 1: Clear queue
        clear_result = await emergency_clear_queue()
        
        # Step 2: Populate cache
        populate_result = await emergency_populate_cache(limit=25)
        
        return {
            'status': 'reset_complete',
            'steps': {
                'queue_cleared': clear_result,
                'cache_populated': populate_result
            },
            'system_ready': populate_result.get('status') in ['success', 'fallback'],
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Emergency reset failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/emergency/test-polygon")
async def test_polygon():
    """
    Test Polygon API response encoding to diagnose compression issues
    """
    try:
        from backend.src.services.polygon_client_fixed import session as poly_session
        
        # Test a simple endpoint to check response encoding
        response = poly_session.get("https://api.polygon.io/v2/reference/exchanges", timeout=20)
        
        # Check content encoding and type
        content_encoding = response.headers.get("Content-Encoding", "none")
        content_type = response.headers.get("Content-Type", "unknown")
        
        # Get sample of raw content (as hex) for diagnosis
        sample_hex = response.content[:16].hex() if response.content else "empty"
        
        # Check if content looks compressed
        is_gzipped = response.content.startswith(b"\x1f\x8b") if response.content else False
        is_zlib = any(response.content.startswith(h) for h in [b"\x78\x01", b"\x78\x9c", b"\x78\xda"]) if response.content else False
        
        return {
            "status_code": response.status_code,
            "content_encoding": content_encoding,
            "content_type": content_type,
            "sample_hex": sample_hex,
            "content_length": len(response.content) if response.content else 0,
            "compression_detected": {
                "gzip": is_gzipped,
                "zlib": is_zlib,
                "header_declares": content_encoding != "none"
            },
            "headers": dict(response.headers),
            "safe_parsing_test": "success" if response.status_code == 200 else "failed"
        }
        
    except Exception as e:
        logger.error(f"Polygon encoding test failed: {e}")
        return {
            "status": "error",
            "error": str(e),
            "message": "Failed to test Polygon API encoding"
        }