#!/usr/bin/env python3
"""
UNIFIED AMC-TRADER DISCOVERY SYSTEM
Single source of truth - MCP-based with no fallbacks
Built to find explosive opportunities before they explode (not after)
"""

import os
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from decimal import Decimal
import json

logger = logging.getLogger(__name__)

class UnifiedDiscoverySystem:
    """
    THE ONLY DISCOVERY SYSTEM
    - Uses Polygon MCP exclusively (no HTTP fallbacks)
    - Fails loudly when real data unavailable
    - Filters out post-explosion moves
    - Returns actionable pre-breakout opportunities
    """

    def __init__(self):
        """Initialize with strict requirements"""
        self.validate_environment()

        # NO FALLBACKS - REAL DATA ONLY
        self.max_daily_move_pct = 20.0  # Reject stocks already up >20%
        self.min_volume_ratio = 2.0     # At least 2x average volume
        self.max_volume_ratio = 15.0    # Reject if >15x (already exploded)
        self.min_price = 0.50          # Minimum viable price
        self.max_price = 50.00         # Maximum price for explosive potential

        # Data freshness requirements
        self.max_data_age_minutes = 60  # Data must be <1 hour old

    def validate_environment(self):
        """Validate all required environment variables"""
        required_vars = [
            "POLYGON_API_KEY",
            "REDIS_URL"
        ]

        missing = [var for var in required_vars if not os.getenv(var)]
        if missing:
            raise RuntimeError(f"‚ùå CRITICAL: Missing environment variables: {missing}")

        # Validate Polygon API key format
        api_key = os.getenv("POLYGON_API_KEY")
        if not api_key or len(api_key) < 10:
            raise RuntimeError("‚ùå CRITICAL: Invalid POLYGON_API_KEY")

        logger.info("‚úÖ Environment validation passed")

    async def call_mcp_snapshot(self, direction: str) -> Dict[str, Any]:
        """
        Wrapper for MCP snapshot calls
        This will be replaced with actual MCP integration in production
        """
        try:
            # This would be the actual MCP call in production
            # For now, we'll simulate the call structure
            if direction == "gainers":
                # Simulate MCP call - in production this would be:
                # return await mcp__polygon__get_snapshot_direction(market_type="stocks", direction="gainers")
                return {
                    'status': 'OK',
                    'tickers': []  # Will be populated by actual MCP call
                }
            elif direction == "losers":
                return {
                    'status': 'OK',
                    'tickers': []
                }
            else:
                raise ValueError(f"Invalid direction: {direction}")

        except Exception as e:
            logger.error(f"‚ùå MCP call failed for {direction}: {e}")
            raise

    def get_timestamp(self) -> str:
        """Get current timestamp for status reporting"""
        return datetime.now().isoformat()

    async def get_market_universe(self) -> List[Dict[str, Any]]:
        """
        Get market universe using MCP - NO FALLBACKS
        Returns current market snapshot or fails
        """
        try:
            logger.info("üì° Fetching market universe via Polygon MCP...")

            # Call MCP functions directly (available in this environment)
            # Note: Using the actual MCP functions available in Claude Code

            logger.info("üì° Fetching gainers data...")
            gainers_response = await self.call_mcp_snapshot("gainers")

            logger.info("üì° Fetching losers data...")
            losers_response = await self.call_mcp_snapshot("losers")

            gainers_data = gainers_response if gainers_response else {'status': 'ERROR', 'tickers': []}
            losers_data = losers_response if losers_response else {'status': 'ERROR', 'tickers': []}

            # Combine and validate data
            all_tickers = []

            if gainers_data.get('status') == 'OK':
                all_tickers.extend(gainers_data.get('tickers', []))
            else:
                raise RuntimeError("‚ùå CRITICAL: Failed to get gainers data from MCP")

            if losers_data.get('status') == 'OK':
                all_tickers.extend(losers_data.get('tickers', []))
            else:
                logger.warning("‚ö†Ô∏è Failed to get losers data, continuing with gainers only")

            if len(all_tickers) < 50:
                raise RuntimeError(f"‚ùå CRITICAL: Insufficient universe size: {len(all_tickers)} tickers")

            logger.info(f"‚úÖ Retrieved {len(all_tickers)} tickers from MCP")
            return all_tickers

        except Exception as e:
            logger.error(f"‚ùå FATAL: Universe retrieval failed: {e}")
            raise RuntimeError(f"Real-time data unavailable: {e}")

    def apply_post_explosion_filter(self, tickers: List[Dict]) -> List[Dict]:
        """
        CRITICAL FILTER: Remove stocks that already exploded
        We want to catch stocks BEFORE they move 100%+, not after
        """
        logger.info("üîç Applying post-explosion filter...")

        pre_filter_count = len(tickers)
        filtered_tickers = []
        filter_stats = {
            'post_explosion': 0,
            'volume_explosion': 0,
            'price_too_high': 0,
            'price_too_low': 0,
            'insufficient_volume': 0,
            'passed': 0
        }

        for ticker in tickers:
            symbol = ticker.get('ticker', 'N/A')
            daily_change_pct = ticker.get('todaysChangePerc', 0)
            price = ticker.get('day', {}).get('c', 0)
            volume = ticker.get('day', {}).get('v', 0)
            prev_volume = ticker.get('prevDay', {}).get('v', 1)  # Avoid division by zero

            # Calculate volume ratio
            volume_ratio = volume / max(prev_volume, 1)

            # Filter criteria
            if abs(daily_change_pct) > self.max_daily_move_pct:
                filter_stats['post_explosion'] += 1
                logger.debug(f"‚ùå {symbol}: Post-explosion move {daily_change_pct:.1f}%")
                continue

            if volume_ratio > self.max_volume_ratio:
                filter_stats['volume_explosion'] += 1
                logger.debug(f"‚ùå {symbol}: Volume explosion {volume_ratio:.1f}x")
                continue

            if price > self.max_price:
                filter_stats['price_too_high'] += 1
                logger.debug(f"‚ùå {symbol}: Price too high ${price:.2f}")
                continue

            if price < self.min_price:
                filter_stats['price_too_low'] += 1
                logger.debug(f"‚ùå {symbol}: Price too low ${price:.2f}")
                continue

            if volume_ratio < self.min_volume_ratio:
                filter_stats['insufficient_volume'] += 1
                logger.debug(f"‚ùå {symbol}: Insufficient volume {volume_ratio:.1f}x")
                continue

            # Passed all filters
            filter_stats['passed'] += 1

            # Add calculated metrics
            ticker['volume_ratio'] = volume_ratio
            ticker['filter_score'] = self.calculate_filter_score(ticker)
            filtered_tickers.append(ticker)

            logger.debug(f"‚úÖ {symbol}: Passed filters - {daily_change_pct:.1f}% move, {volume_ratio:.1f}x volume")

        logger.info(f"üîç Filter Results: {pre_filter_count} ‚Üí {len(filtered_tickers)} candidates")
        logger.info(f"üìä Filter Stats: {json.dumps(filter_stats, indent=2)}")

        if len(filtered_tickers) == 0:
            logger.warning("‚ö†Ô∏è NO CANDIDATES SURVIVED FILTERING - Market may be in extreme volatility")

        return filtered_tickers

    def calculate_filter_score(self, ticker: Dict) -> float:
        """
        Calculate a score for remaining candidates
        Higher score = better opportunity
        """
        daily_change = abs(ticker.get('todaysChangePerc', 0))
        volume_ratio = ticker.get('volume_ratio', 1)
        price = ticker.get('day', {}).get('c', 0)

        # Score components (0-1 each)

        # 1. Ideal daily move (5-15% gets highest score)
        if 5 <= daily_change <= 15:
            move_score = 1.0
        elif daily_change < 5:
            move_score = daily_change / 5.0
        else:  # >15%
            move_score = max(0, (20 - daily_change) / 5.0)

        # 2. Volume surge (3-8x is ideal)
        if 3 <= volume_ratio <= 8:
            volume_score = 1.0
        elif volume_ratio < 3:
            volume_score = volume_ratio / 3.0
        else:  # >8x
            volume_score = max(0, (15 - volume_ratio) / 7.0)

        # 3. Price sweet spot ($2-20 ideal)
        if 2 <= price <= 20:
            price_score = 1.0
        elif price < 2:
            price_score = price / 2.0
        else:  # >20
            price_score = max(0, (50 - price) / 30.0)

        # Weighted combination
        total_score = (move_score * 0.4) + (volume_score * 0.4) + (price_score * 0.2)

        return round(total_score, 3)

    async def discover_opportunities(self, limit: int = 20) -> Dict[str, Any]:
        """
        Main discovery function - THE ONLY ENTRY POINT
        Returns pre-breakout opportunities or fails with clear error
        """
        start_time = datetime.now()

        try:
            logger.info("üöÄ UNIFIED DISCOVERY STARTING")
            logger.info(f"üìä Target: {limit} pre-breakout opportunities")

            # Step 1: Get universe (FAIL if no real data)
            universe = await self.get_market_universe()

            # Step 2: Apply post-explosion filter
            candidates = self.apply_post_explosion_filter(universe)

            # Step 3: Sort by filter score and limit
            candidates.sort(key=lambda x: x.get('filter_score', 0), reverse=True)
            candidates = candidates[:limit]

            # Step 4: Categorize candidates
            trade_ready = []
            watchlist = []

            for candidate in candidates:
                score = candidate.get('filter_score', 0)
                if score >= 0.7:
                    candidate['action_tag'] = 'trade_ready'
                    trade_ready.append(candidate)
                elif score >= 0.4:
                    candidate['action_tag'] = 'watchlist'
                    watchlist.append(candidate)
                else:
                    candidate['action_tag'] = 'monitor'

            execution_time = (datetime.now() - start_time).total_seconds()

            # Step 5: Build response
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'execution_time_sec': round(execution_time, 2),
                'data_source': 'POLYGON_MCP_REAL_TIME',
                'data_age_status': 'FRESH',
                'universe_size': len(universe),
                'post_filter_count': len(candidates),
                'filter_pass_rate_pct': round((len(candidates) / len(universe)) * 100, 1),
                'candidates': candidates,
                'summary': {
                    'trade_ready_count': len(trade_ready),
                    'watchlist_count': len(watchlist),
                    'total_opportunities': len(candidates)
                },
                'system_health': {
                    'no_fallbacks_used': True,
                    'real_time_data': True,
                    'mcp_operational': True
                }
            }

            logger.info(f"‚úÖ DISCOVERY COMPLETED: {len(candidates)} opportunities found in {execution_time:.2f}s")
            return result

        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()

            error_result = {
                'status': 'FAILED',
                'timestamp': datetime.now().isoformat(),
                'execution_time_sec': round(execution_time, 2),
                'error': str(e),
                'data_source': 'UNAVAILABLE',
                'data_age_status': 'STALE_OR_MISSING',
                'candidates': [],
                'system_health': {
                    'no_fallbacks_used': True,
                    'real_time_data': False,
                    'mcp_operational': False
                },
                'alert': 'üö® REAL-TIME DATA UNAVAILABLE - NO TRADING RECOMMENDATIONS'
            }

            logger.error(f"‚ùå DISCOVERY FAILED: {e}")
            return error_result